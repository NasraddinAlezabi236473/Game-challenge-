{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "972fcc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='qwen3:32b', modified_at=datetime.datetime(2025, 4, 30, 17, 29, 13, 819781, tzinfo=TzInfo(UTC)), digest='e1c9f234c6eb37545e991032e3d674e0762763cddab4e69009a2f0d8739deaaf', size=20201253588, details=ModelDetails(parent_model='', format='gguf', family='qwen3', families=['qwen3'], parameter_size='32.8B', quantization_level='Q4_K_M')), Model(model='mxbai-embed-large:latest', modified_at=datetime.datetime(2025, 4, 29, 17, 29, 15, 525782, tzinfo=TzInfo(UTC)), digest='468836162de7f81e041c43663fedbbba921dcea9b9fefea135685a39b2d83dd8', size=669615493, details=ModelDetails(parent_model='', format='gguf', family='bert', families=['bert'], parameter_size='334M', quantization_level='F16')), Model(model='mistral-small:22b-instruct-2409-q5_K_M', modified_at=datetime.datetime(2025, 4, 28, 9, 6, 26, 140482, tzinfo=TzInfo(UTC)), digest='eec48ef9afdc2886835d1caba847908b7268e18e8c1c886ef7620b519e959fcb', size=15722567855, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='22.2B', quantization_level='Q5_K_M')), Model(model='gemma3:27b', modified_at=datetime.datetime(2025, 4, 28, 8, 52, 13, 196502, tzinfo=TzInfo(UTC)), digest='a418f5838eaf7fe2cfe0a3046c8384b68ba43a4435542c942f9db00a5f342203', size=17396936941, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='27.4B', quantization_level='Q4_K_M')), Model(model='gemma2:27b-instruct-q5_K_M', modified_at=datetime.datetime(2025, 4, 28, 8, 25, 39, 864539, tzinfo=TzInfo(UTC)), digest='fc62fd7d70c939f920e95ded7ffde3407174a7c32e3d0ee1b19512bb5e104915', size=19408127108, details=ModelDetails(parent_model='', format='gguf', family='gemma2', families=['gemma2'], parameter_size='27.2B', quantization_level='Q5_K_M')), Model(model='qwq:32b', modified_at=datetime.datetime(2025, 4, 28, 8, 17, 12, 452551, tzinfo=TzInfo(UTC)), digest='009cb3f08d74437380f3b84194c1bd34f1cc3d95a2bb87241d89387fc22a9ddf', size=19851349657, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='32.8B', quantization_level='Q4_K_M')), Model(model='llama3.1:70b-instruct-q5_K_M', modified_at=datetime.datetime(2025, 4, 28, 8, 13, 47, 232556, tzinfo=TzInfo(UTC)), digest='1201b5de86509d91378ae21503d9a22cd347d3ce8cabf74c41475c416539ddd3', size=49949835665, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='70.6B', quantization_level='Q5_K_M')), Model(model='qwen2.5:72b-instruct-q5_K_M', modified_at=datetime.datetime(2025, 4, 28, 8, 4, 24, 616569, tzinfo=TzInfo(UTC)), digest='210a146b67de6e21da50c673628ee42e753d6de88ca12066c41e5e9c1da6c6a3', size=54447475281, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='72.7B', quantization_level='Q5_K_M')), Model(model='deepseek-r1:70b', modified_at=datetime.datetime(2025, 4, 28, 7, 44, 31, 44596, tzinfo=TzInfo(UTC)), digest='0c1615a8ca32ef41e433aa420558b4685f9fc7f3fd74119860a8e2e389cd7942', size=42520397704, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='70.6B', quantization_level='Q4_K_M')), Model(model='qwen2.5:32b-instruct-q4_1', modified_at=datetime.datetime(2025, 4, 25, 14, 26, 4, 662042, tzinfo=TzInfo(UTC)), digest='c35507089a57bef7b5936876775a7820c129896f0079c5841e7e1f9ee2c0e953', size=20639256227, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='32.8B', quantization_level='Q4_1')), Model(model='llama3.2:latest', modified_at=datetime.datetime(2025, 4, 25, 13, 9, 19, 618149, tzinfo=TzInfo(UTC)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M')), Model(model='llama3.1:70b-instruct-q4_K_M', modified_at=datetime.datetime(2025, 4, 25, 9, 30, 31, 734453, tzinfo=TzInfo(UTC)), digest='711a9e8463afd8edd580debd3b5c521521ebe55ba95bb80d576f4149969e07c6', size=42520412561, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='70.6B', quantization_level='Q4_K_M')), Model(model='deepseek-r1:8b', modified_at=datetime.datetime(2025, 4, 24, 13, 22, 28, 416132, tzinfo=TzInfo(UTC)), digest='28f8fd6cdc677661426adab9338ce3c013d7e69a5bea9e704b364171a5d61a10', size=4920738407, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M')), Model(model='deepseek-r1:1.5b', modified_at=datetime.datetime(2025, 4, 24, 10, 31, 48, 360369, tzinfo=TzInfo(UTC)), digest='a42b25d8c10a841bd24724309898ae851466696a7d7f3a0a408b895538ccbc96', size=1117322599, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='1.8B', quantization_level='Q4_K_M')), Model(model='qwen2.5:7b', modified_at=datetime.datetime(2025, 4, 23, 13, 0, 0, 710164, tzinfo=TzInfo(UTC)), digest='845dbda0ea48ed749caafd9e6037047aa19acfcfd82e704d7ca97d631a0b697e', size=4683087332, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='7.6B', quantization_level='Q4_K_M')), Model(model='llama3.1:8b-instruct-q8_0', modified_at=datetime.datetime(2025, 4, 18, 17, 15, 58, 867815, tzinfo=TzInfo(UTC)), digest='b158ded76fa05be6bce8a682099ce5df8c5571340a04cf63a2923464679db576', size=8540789934, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q8_0')), Model(model='command-r:latest', modified_at=datetime.datetime(2025, 4, 17, 16, 17, 38, 765897, tzinfo=TzInfo(UTC)), digest='7d96360d357f979f63a2be43d5bde200bfcbead950a4072e805d2196ae589aaa', size=18719506654, details=ModelDetails(parent_model='', format='gguf', family='command-r', families=['command-r'], parameter_size='32.3B', quantization_level='Q4_0')), Model(model='deepseek-r1:32b', modified_at=datetime.datetime(2025, 4, 17, 10, 23, 33, 974389, tzinfo=TzInfo(UTC)), digest='38056bbcbb2d068501ecb2d5ea9cea9dd4847465f1ab88c4d4a412a9f7792717', size=19851337640, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='32.8B', quantization_level='Q4_K_M')), Model(model='deepseek-r1:14b', modified_at=datetime.datetime(2025, 4, 17, 10, 19, 27, 582395, tzinfo=TzInfo(UTC)), digest='ea35dfe18182f635ee2b214ea30b7520fe1ada68da018f8b395b444b662d4f1a', size=8988112040, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='14.8B', quantization_level='Q4_K_M')), Model(model='nemotron-mini:latest', modified_at=datetime.datetime(2025, 4, 17, 1, 40, 29, 235116, tzinfo=TzInfo(UTC)), digest='ed76ab18784f5a01c9ec5b3c250e964d4f9c7a983e59ba041bb995f0fe2e8fb3', size=2697402546, details=ModelDetails(parent_model='', format='gguf', family='nemotron', families=['nemotron'], parameter_size='4.2B', quantization_level='Q4_K_M')), Model(model='llava:13b', modified_at=datetime.datetime(2025, 4, 16, 19, 27, 49, 383634, tzinfo=TzInfo(UTC)), digest='0d0eb4d7f485d7d0a21fd9b0c1d5b04da481d2150a097e81b64acb59758fdef6', size=8011256494, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama', 'clip'], parameter_size='13B', quantization_level='Q4_0')), Model(model='llava:7b', modified_at=datetime.datetime(2025, 4, 16, 19, 21, 17, 799643, tzinfo=TzInfo(UTC)), digest='8dd30f6b0cb19f555f2c7a7ebda861449ea2cc76bf1f44e262931f45fc81d081', size=4733363377, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama', 'clip'], parameter_size='7B', quantization_level='Q4_0')), Model(model='llama3.2:3b', modified_at=datetime.datetime(2025, 4, 16, 13, 48, 18, 116106, tzinfo=TzInfo(UTC)), digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72', size=2019393189, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M')), Model(model='llama3.1:8b', modified_at=datetime.datetime(2025, 4, 15, 10, 12, 54, 482407, tzinfo=TzInfo(UTC)), digest='46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e', size=4920753328, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M')), Model(model='llama3:latest', modified_at=datetime.datetime(2025, 4, 14, 18, 2, 33, 359755, tzinfo=TzInfo(UTC)), digest='365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1', size=4661224676, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_0')), Model(model='llama3:70b-instruct-q5_K_M', modified_at=datetime.datetime(2025, 4, 14, 13, 9, 8, 144163, tzinfo=TzInfo(UTC)), digest='4e84a551486247fcb27ce919d2770b0c1b468768a373c250d3c4708a83a394ed', size=49949829575, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='70.6B', quantization_level='Q5_K_M')), Model(model='llama3.3:70b-instruct-q5_K_M', modified_at=datetime.datetime(2024, 12, 9, 18, 36, 32, 420565, tzinfo=TzInfo(UTC)), digest='a495e09a05137d0216af9d190425f9c8fd7fa15e5dd581c928e221c3861126b9', size=49949837020, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='70.6B', quantization_level='Q5_K_M')), Model(model='llama3.3:70b', modified_at=datetime.datetime(2024, 12, 9, 18, 17, 20, 176592, tzinfo=TzInfo(UTC)), digest='a6eb4748fd2990ad2952b2335a95a7f952d1a06119a0aa6a2df6cd052a93a3fa', size=42520413916, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='70.6B', quantization_level='Q4_K_M')), Model(model='llava:34b', modified_at=datetime.datetime(2024, 10, 22, 13, 38, 17, 122724, tzinfo=TzInfo(UTC)), digest='3d2d24f4667475bd28d515495b0dcc03b5a951be261a0babdb82087fc11620ee', size=20166497526, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama', 'clip'], parameter_size='34B', quantization_level='Q4_0')), Model(model='gemma2:27b', modified_at=datetime.datetime(2024, 10, 22, 13, 38, 15, 682724, tzinfo=TzInfo(UTC)), digest='53261bc9c192c1cb5fcc898dd3aa15da093f5ab6f08e17e48cf838bb1c58abfe', size=15628387458, details=ModelDetails(parent_model='', format='gguf', family='gemma2', families=['gemma2'], parameter_size='27.2B', quantization_level='Q4_0'))])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import Client\n",
    "ollama_client = Client(host=\"http://194.171.191.226:3061\")\n",
    "ollama_client.list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b3e7f9",
   "metadata": {},
   "source": [
    "### Clau-hee chatbot interaction initial test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4ba574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Compliance Challenge!\n",
      "Let's test your knowledge of our Gift Policy. Answer True or False.\n",
      "You'll be asked 5 questions.... on our Gift Policy.\n",
      "One wrong move.. you will be eliminated!\n",
      "Answer 5 questions in a row to win, think smart!\n",
      "Let the game begin!!\n"
     ]
    }
   ],
   "source": [
    "print('Welcome to the Compliance Challenge!')\n",
    "print(\"Let's test your knowledge of our Gift Policy. Answer True or False.\")\n",
    "print(\"You'll be asked 5 questions.... on our Gift Policy.\")\n",
    "print(\"One wrong move.. you will be eliminated!\")\n",
    "print(\"Answer 5 questions in a row to win, think smart!\")\n",
    "print(\"Let the game begin!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde97f17",
   "metadata": {},
   "source": [
    "## Compliance Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5918b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    {\n",
    "        \"question\": \"Based on the Gift Policy, it's acceptable to give a small gift in cash to a vendor during holiday season.\",\n",
    "        \"answer\": False,\n",
    "        \"level\": \"easy\",\n",
    "        \"explanation\": \"The policy prohibits cash or cash equivalents, regardless of the amount or occasion.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"A promotional mug worth $10, given at a public product launch event, is allowed under the Gift Policy.\",\n",
    "        \"answer\": True,\n",
    "        \"level\": \"easy\",\n",
    "        \"explanation\": \"The gift is reasonable, below the threshold, openly given, and part of a legitimate business event.\"\n",
    "    },\n",
    "    {\n",
    "       \"question\": \"According to the Gift Policy, giving a gift in secret is acceptable if the value is within the allowed threshold.\",\n",
    "        \"answer\": False,\n",
    "        \"level\": \"medium\",\n",
    "        \"explanation\": \"Gifts must be given openly. Secrecy violates the policy even if the value is under the threshold.\"\n",
    "    }, \n",
    "    {\n",
    "       \"question\": \"Sponsorships can include providing your organization's products or services as in-kind contributions to a third party, with the expectation of brand association.\",\n",
    "        \"answer\": True,\n",
    "        \"level\": \"hard\",\n",
    "        \"explanation\": \"Sponsorships involve in-kind contributions to promote brand association with an event or activity.\"\n",
    "    },  \n",
    "    {\n",
    "       \"question\": \"Donations are voluntary payments in the form of money, in-kind contributions, or anything of value to a non-profit or charitable organization, with no expectation of receiving a tangible benefit in return.\",\n",
    "       \"answer\": True,\n",
    "       \"level\": \"hard\",\n",
    "       \"explanation\": \"Donations are made without expecting any measurable benefit or service in return.\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40d95f",
   "metadata": {},
   "source": [
    "#### Building dialogue logic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7bb740",
   "metadata": {},
   "source": [
    "Presents a question  \n",
    "Easy medium to difficulty   \n",
    "Returns if the user incorrectly answers  \n",
    "5 streak win, wins the game!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21150c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clau_hee_response(user_input, question_text, explanation_text, correct):\n",
    "    context_intro = (\n",
    "        \"You are Clau-Hee, a sarcastic and smart compliance training bot. \"\n",
    "        \"You're guiding users through a Squid Game-style quiz about the gift policy. \"\n",
    "        \"Be witty but helpful. Never lie.\"\n",
    "    )\n",
    "\n",
    "    if correct:\n",
    "        user_msg = (\n",
    "            f\"The player answered '{user_input}' to the question: '{question_text}'. \"\n",
    "            f\"The answer was correct. The explanation is: {explanation_text}\"\n",
    "        )\n",
    "    else:\n",
    "        user_msg = (\n",
    "            f\"The player answered '{user_input}' to the question: '{question_text}'. \"\n",
    "            f\"The answer was incorrect. The correct explanation is: {explanation_text}\"\n",
    "        )\n",
    "\n",
    "    response = ollama_client.chat(\n",
    "        model=\"llama3.1:8b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": context_intro},\n",
    "            {\"role\": \"user\", \"content\": user_msg}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response[\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff5154cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(q):\n",
    "    print(f\"\\nClau-Hee: {q['question']}\")\n",
    "    user_input = input(\"Your answer (True/False): \").strip().lower()\n",
    "\n",
    "    while user_input not in [\"true\", \"false\"]:\n",
    "        user_input = input(\"Please type True or False: \").strip().lower()\n",
    "\n",
    "    is_correct = (user_input == \"true\" and q[\"answer\"] is True) or (user_input == \"false\" and q[\"answer\"] is False)\n",
    "\n",
    "    # Generate Clau-Hee's dynamic sarcastic response\n",
    "    feedback = clau_hee_response(\n",
    "        user_input=user_input,\n",
    "        question_text=q[\"question\"],\n",
    "        explanation_text=q[\"explanation\"],\n",
    "        correct=is_correct\n",
    "    )\n",
    "\n",
    "    print(f\"\\nClau-Hee says: {feedback}\")\n",
    "    return is_correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57baa7a",
   "metadata": {},
   "source": [
    "## compliance quiz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bebc7705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1: Easy\n",
      "\n",
      "Clau-Hee: Based on the Gift Policy, it's acceptable to give a small gift in cash to a vendor during holiday season.\n",
      "\n",
      "Clau-Hee says: You think you're so clever, don't you? Thinking you can get away with giving a little \"gift\" to that vendor friend of yours during the holidays. Well, let me tell you, friend, you'd be wrong... and also probably in trouble.\n",
      "\n",
      "The Gift Policy is crystal clear: no cash or cash equivalents allowed, regardless of how small or what occasion it's for. So, that $20 bill you were planning to slip to your favorite vendor? Not exactly a winning move in this game.\n",
      "\n",
      "You got this one right, by the way. Now, let's see if you can keep up the good work and make it through the rest of the quiz without... \"gift-ing\" yourself an F.\n",
      "\n",
      "Question 2: Easy\n",
      "\n",
      "Clau-Hee: A promotional mug worth $10, given at a public product launch event, is allowed under the Gift Policy.\n",
      "\n",
      "Clau-Hee says: Well, well, well. Look who's finally getting it right! You think you're the real MVP (Most Valuable Participant) of this quiz, don't you? But let me tell you, just because you answered correctly doesn't mean you get to breathe a sigh of relief... yet.\n",
      "\n",
      "Now that we've got your answer out of the way, let's break down why it was correct. See, our Gift Policy is like a fine-tuned machine (okay, maybe not fine-tuned, but at least it's got some decent guidelines). And what you did right was:\n",
      "\n",
      "1. **Reasonable**: The mug is worth $10, which is below the gift threshold (which we won't mention here to avoid spoiling all the fun).\n",
      "2. **Below the threshold**: Yep, that mug stays under the radar – literally and figuratively.\n",
      "3. **Openly given**: You didn't sneakily hand it out in a back alley or something (although, if you did, I'm sure we'd have some interesting company at our compliance meetings). No, this was part of an official event where everyone knew what was up.\n",
      "4. **Legitimate business event**: It wasn't just some excuse to give away free swag (although, who wouldn't want that?).\n",
      "\n",
      "Great job! You managed not to get caught in the \" Gift Policy Trap\" – at least for now. But remember, complacency is a slippery slope...\n",
      "\n",
      "Question 3: Medium\n",
      "\n",
      "Clau-Hee: According to the Gift Policy, giving a gift in secret is acceptable if the value is within the allowed threshold.\n",
      "\n",
      "Clau-Hee says: You think you're so clever, don't you? Well, let's see how long your \"winning streak\" lasts.\n",
      "\n",
      "Correct again! You got it right. I guess you didn't get caught up in the excitement of trying to sneak a gift past the policy. Secrecy is still not allowed, even if you're trying to keep it on the down-low like a secret agent (but without the cool gadgets).\n",
      "\n",
      "You're one step closer to escaping this quiz alive! What's your next move?\n",
      "\n",
      "Question 4: Hard\n",
      "\n",
      "Clau-Hee: Sponsorships can include providing your organization's products or services as in-kind contributions to a third party, with the expectation of brand association.\n",
      "\n",
      "Clau-Hee says: Poor player. Thought they could sneak by without getting caught. Let me enlighten you:\n",
      "\n",
      "\"In-kind contributions\" are like offering a free pass to the front of the line, but instead of a physical prize, it's your company's products or services. It's a way for your organization to associate with an event or activity and get some good publicity, like a sweet, sweet selfie with the brand ambassador.\n",
      "\n",
      "Don't worry, it's not all doom and gloom. You can still learn from this mistake. Remember, sponsorships are all about promoting brand association through in-kind contributions. Got it?\n",
      "\n",
      "What's your next move, player?\n",
      "\n",
      "Clau-Hee: Game over! Try again next time.\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "history = []\n",
    "\n",
    "for i, q in enumerate(questions[:5]):  # Ask exactly 5 questions from your list\n",
    "    print(f\"\\nQuestion {i + 1}: {q['level'].capitalize()}\")\n",
    "    result = ask_question(q)\n",
    "\n",
    "    history.append({\n",
    "        \"question\": q[\"question\"],\n",
    "        \"user_answer\": \"true\" if result else \"false\",\n",
    "        \"correct\": result,\n",
    "        \"explanation\": q[\"explanation\"]\n",
    "    })\n",
    "\n",
    "    if result:\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        print(\"\\nClau-Hee: Game over! Try again next time.\")\n",
    "        break\n",
    "\n",
    "if correct_count == 5:\n",
    "    print(\"\\nClau-Hee: 🎉 CONGRATULATIONS! You answered all 5 questions correctly.\")\n",
    "    print(\"Clau-Hee: You are now a Compliance Champion! 🏆\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e500aa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summary ---\n",
      "Q1: ✅ Correct – Based on the Gift Policy, it's acceptable to give a small gift in cash to a vendor during holiday season.\n",
      "Q2: ✅ Correct – A promotional mug worth $10, given at a public product launch event, is allowed under the Gift Policy.\n",
      "Q3: ✅ Correct – According to the Gift Policy, giving a gift in secret is acceptable if the value is within the allowed threshold.\n",
      "Q4: ❌ Incorrect – Sponsorships can include providing your organization's products or services as in-kind contributions to a third party, with the expectation of brand association.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Summary ---\")\n",
    "for i, entry in enumerate(history):\n",
    "    status = \"✅ Correct\" if entry[\"correct\"] else \"❌ Incorrect\"\n",
    "    print(f\"Q{i+1}: {status} – {entry['question']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61485682",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
